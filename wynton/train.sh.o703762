2022-09-27 20:42:39.858332: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-27 20:42:41.631134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43672 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:01:00.0, compute capability: 8.6
2022-09-27 20:42:41.633897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 43672 MB memory:  -> device: 1, name: NVIDIA A40, pci bus id: 0000:41:00.0, compute capability: 8.6
2022-09-27 20:42:41.635078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 43672 MB memory:  -> device: 2, name: NVIDIA A40, pci bus id: 0000:81:00.0, compute capability: 8.6
2022-09-27 20:42:41.636434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 43672 MB memory:  -> device: 3, name: NVIDIA A40, pci bus id: 0000:c1:00.0, compute capability: 8.6
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
here
Model: "model_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 sequence (InputLayer)          [(None, 2114, 4)]    0           []                               
                                                                                                  
 wo_bias_bpnet_1st_conv (Conv1D  (None, 2094, 512)   43520       ['sequence[0][0]']               
 )                                                                                                
                                                                                                  
 wo_bias_bpnet_1conv (Conv1D)   (None, 2090, 512)    786944      ['wo_bias_bpnet_1st_conv[0][0]'] 
                                                                                                  
 wo_bias_bpnet_1crop (Cropping1  (None, 2090, 512)   0           ['wo_bias_bpnet_1st_conv[0][0]'] 
 D)                                                                                               
                                                                                                  
 add (Add)                      (None, 2090, 512)    0           ['wo_bias_bpnet_1conv[0][0]',    
                                                                  'wo_bias_bpnet_1crop[0][0]']    
                                                                                                  
 wo_bias_bpnet_2conv (Conv1D)   (None, 2082, 512)    786944      ['add[0][0]']                    
                                                                                                  
 wo_bias_bpnet_2crop (Cropping1  (None, 2082, 512)   0           ['add[0][0]']                    
 D)                                                                                               
                                                                                                  
 add_1 (Add)                    (None, 2082, 512)    0           ['wo_bias_bpnet_2conv[0][0]',    
                                                                  'wo_bias_bpnet_2crop[0][0]']    
                                                                                                  
 wo_bias_bpnet_3conv (Conv1D)   (None, 2066, 512)    786944      ['add_1[0][0]']                  
                                                                                                  
 wo_bias_bpnet_3crop (Cropping1  (None, 2066, 512)   0           ['add_1[0][0]']                  
 D)                                                                                               
                                                                                                  
 add_2 (Add)                    (None, 2066, 512)    0           ['wo_bias_bpnet_3conv[0][0]',    
                                                                  'wo_bias_bpnet_3crop[0][0]']    
                                                                                                  
 wo_bias_bpnet_4conv (Conv1D)   (None, 2034, 512)    786944      ['add_2[0][0]']                  
                                                                                                  
 wo_bias_bpnet_4crop (Cropping1  (None, 2034, 512)   0           ['add_2[0][0]']                  
 D)                                                                                               
                                                                                                  
 add_3 (Add)                    (None, 2034, 512)    0           ['wo_bias_bpnet_4conv[0][0]',    
                                                                  'wo_bias_bpnet_4crop[0][0]']    
                                                                                                  
 wo_bias_bpnet_5conv (Conv1D)   (None, 1970, 512)    786944      ['add_3[0][0]']                  
                                                                                                  
 wo_bias_bpnet_5crop (Cropping1  (None, 1970, 512)   0           ['add_3[0][0]']                  
 D)                                                                                               
                                                                                                  
 add_4 (Add)                    (None, 1970, 512)    0           ['wo_bias_bpnet_5conv[0][0]',    
                                                                  'wo_bias_bpnet_5crop[0][0]']    
                                                                                                  
 wo_bias_bpnet_6conv (Conv1D)   (None, 1842, 512)    786944      ['add_4[0][0]']                  
                                                                                                  
 wo_bias_bpnet_6crop (Cropping1  (None, 1842, 512)   0           ['add_4[0][0]']                  
 D)                                                                                               
                                                                                                  
 add_5 (Add)                    (None, 1842, 512)    0           ['wo_bias_bpnet_6conv[0][0]',    
                                                                  'wo_bias_bpnet_6crop[0][0]']    
                                                                                                  
 wo_bias_bpnet_7conv (Conv1D)   (None, 1586, 512)    786944      ['add_5[0][0]']                  
                                                                                                  
 wo_bias_bpnet_7crop (Cropping1  (None, 1586, 512)   0           ['add_5[0][0]']                  
 D)                                                                                               
                                                                                                  
 add_6 (Add)                    (None, 1586, 512)    0           ['wo_bias_bpnet_7conv[0][0]',    
                                                                  'wo_bias_bpnet_7crop[0][0]']    
                                                                                                  
 wo_bias_bpnet_8conv (Conv1D)   (None, 1074, 512)    786944      ['add_6[0][0]']                  
                                                                                                  
 wo_bias_bpnet_8crop (Cropping1  (None, 1074, 512)   0           ['add_6[0][0]']                  
 D)                                                                                               
                                                                                                  
 add_7 (Add)                    (None, 1074, 512)    0           ['wo_bias_bpnet_8conv[0][0]',    
                                                                  'wo_bias_bpnet_8crop[0][0]']    
                                                                                                  
 wo_bias_bpnet_prof_out_precrop  (None, 1000, 1)     38401       ['add_7[0][0]']                  
  (Conv1D)                                                                                        
                                                                                                  
 wo_bias_bpnet_logitt_before_fl  (None, 1000, 1)     0           ['wo_bias_bpnet_prof_out_precrop[
 atten (Cropping1D)                                              0][0]']                          
                                                                                                  
 gap (GlobalAveragePooling1D)   (None, 512)          0           ['add_7[0][0]']                  
                                                                                                  
 wo_bias_bpnet_logits_profile_p  (None, 1000)        0           ['wo_bias_bpnet_logitt_before_fla
 redictions (Flatten)                                            tten[0][0]']                     
                                                                                                  
 wo_bias_bpnet_logcount_predict  (None, 1)           513         ['gap[0][0]']                    
 ions (Dense)                                                                                     
                                                                                                  
==================================================================================================
Total params: 6,377,986
Trainable params: 6,377,986
Non-trainable params: 0
__________________________________________________________________________________________________
None
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2114, 4)]    0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2114, 4)]    0           []                               
                                                                                                  
 model_1 (Functional)           [(None, 1000),       6377986     ['input_1[0][0]',                
                                 (None, 1)]                       'input_2[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 1000)         0           ['model_1[0][0]',                
                                                                  'model_1[0][1]']                
                                                                                                  
 lambda_1 (Lambda)              (None, 1000)         0           ['model_1[1][0]',                
                                                                  'model_1[1][1]']                
                                                                                                  
 lambda_2 (Lambda)              (None, 1000)         0           ['lambda[0][0]',                 
                                                                  'lambda_1[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 1000, 1)      0           ['lambda_2[0][0]']               
                                                                                                  
 conv1d (Conv1D)                (None, 991, 16)      176         ['tf.expand_dims[0][0]']         
                                                                                                  
 flatten (Flatten)              (None, 15856)        0           ['conv1d[0][0]']                 
                                                                                                  
 dense (Dense)                  (None, 48)           761136      ['flatten[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 1)            49          ['dense[0][0]']                  
                                                                                                  
==================================================================================================
Total params: 7,139,347
Trainable params: 7,139,347
Non-trainable params: 0
__________________________________________________________________________________________________
None
(135, 2114, 4) (135, 2114, 4) (135,)
LR: 5e-05
Epoch 1/40
2022-09-27 20:42:45.859249: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8201
2022-09-27 20:42:47.649481: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2022-09-27 20:42:47.740382: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6
2022-09-27 20:42:47.740819: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at ptxas
2022-09-27 20:42:47.741749: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-09-27 20:42:49.761357: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
1/7 [===>..........................] - ETA: 52s - loss: 0.1380 - root_mean_squared_error: 0.13802/7 [=======>......................] - ETA: 0s - loss: 0.1571 - root_mean_squared_error: 0.1582 3/7 [===========>..................] - ETA: 0s - loss: 0.1483 - root_mean_squared_error: 0.14974/7 [================>.............] - ETA: 0s - loss: 0.1430 - root_mean_squared_error: 0.14445/7 [====================>.........] - ETA: 0s - loss: 0.1388 - root_mean_squared_error: 0.14026/7 [========================>.....] - ETA: 0s - loss: 0.1357 - root_mean_squared_error: 0.13707/7 [==============================] - ETA: 0s - loss: 0.1370 - root_mean_squared_error: 0.1382
Epoch 1: val_loss improved from inf to 0.11459, saving model to MPRA_model_development/models/MPRAModel.Kampman.mSK_K562.t100.p0.5.c300.v2.conv.16.10.dense.48.dense.1.rmse
2022-09-27 20:42:55.468604: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
7/7 [==============================] - 15s 1s/step - loss: 0.1370 - root_mean_squared_error: 0.1382 - val_loss: 0.1146 - val_root_mean_squared_error: 0.1148
Epoch 2/40
1/7 [===>..........................] - ETA: 0s - loss: 0.1045 - root_mean_squared_error: 0.10452/7 [=======>......................] - ETA: 0s - loss: 0.1072 - root_mean_squared_error: 0.10723/7 [===========>..................] - ETA: 0s - loss: 0.1090 - root_mean_squared_error: 0.10904/7 [================>.............] - ETA: 0s - loss: 0.1118 - root_mean_squared_error: 0.11195/7 [====================>.........] - ETA: 0s - loss: 0.1114 - root_mean_squared_error: 0.11156/7 [========================>.....] - ETA: 0s - loss: 0.1082 - root_mean_squared_error: 0.10867/7 [==============================] - ETA: 0s - loss: 0.1076 - root_mean_squared_error: 0.1079
Epoch 2: val_loss improved from 0.11459 to 0.10768, saving model to MPRA_model_development/models/MPRAModel.Kampman.mSK_K562.t100.p0.5.c300.v2.conv.16.10.dense.48.dense.1.rmse
7/7 [==============================] - 5s 797ms/step - loss: 0.1076 - root_mean_squared_error: 0.1079 - val_loss: 0.1077 - val_root_mean_squared_error: 0.1077
Epoch 3/40
1/7 [===>..........................] - ETA: 0s - loss: 0.0816 - root_mean_squared_error: 0.08162/7 [=======>......................] - ETA: 0s - loss: 0.0805 - root_mean_squared_error: 0.08053/7 [===========>..................] - ETA: 0s - loss: 0.0850 - root_mean_squared_error: 0.08534/7 [================>.............] - ETA: 0s - loss: 0.0845 - root_mean_squared_error: 0.08465/7 [====================>.........] - ETA: 0s - loss: 0.0875 - root_mean_squared_error: 0.08796/7 [========================>.....] - ETA: 0s - loss: 0.0909 - root_mean_squared_error: 0.09157/7 [==============================] - ETA: 0s - loss: 0.0918 - root_mean_squared_error: 0.0923
Epoch 3: val_loss did not improve from 0.10768
7/7 [==============================] - 1s 161ms/step - loss: 0.0918 - root_mean_squared_error: 0.0923 - val_loss: 0.1082 - val_root_mean_squared_error: 0.1082
Epoch 4/40
1/7 [===>..........................] - ETA: 0s - loss: 0.0616 - root_mean_squared_error: 0.06162/7 [=======>......................] - ETA: 0s - loss: 0.0733 - root_mean_squared_error: 0.07423/7 [===========>..................] - ETA: 0s - loss: 0.0731 - root_mean_squared_error: 0.07374/7 [================>.............] - ETA: 0s - loss: 0.0758 - root_mean_squared_error: 0.07645/7 [====================>.........] - ETA: 0s - loss: 0.0766 - root_mean_squared_error: 0.07716/7 [========================>.....] - ETA: 0s - loss: 0.0767 - root_mean_squared_error: 0.07717/7 [==============================] - ETA: 0s - loss: 0.0766 - root_mean_squared_error: 0.0770
Epoch 4: val_loss did not improve from 0.10768
7/7 [==============================] - 1s 160ms/step - loss: 0.0766 - root_mean_squared_error: 0.0770 - val_loss: 0.1096 - val_root_mean_squared_error: 0.1096
Epoch 5/40
1/7 [===>..........................] - ETA: 0s - loss: 0.0818 - root_mean_squared_error: 0.08182/7 [=======>......................] - ETA: 0s - loss: 0.0720 - root_mean_squared_error: 0.07263/7 [===========>..................] - ETA: 0s - loss: 0.0672 - root_mean_squared_error: 0.06804/7 [================>.............] - ETA: 0s - loss: 0.0651 - root_mean_squared_error: 0.06585/7 [====================>.........] - ETA: 0s - loss: 0.0641 - root_mean_squared_error: 0.06476/7 [========================>.....] - ETA: 0s - loss: 0.0632 - root_mean_squared_error: 0.06387/7 [==============================] - ETA: 0s - loss: 0.0606 - root_mean_squared_error: 0.0616
Epoch 5: val_loss did not improve from 0.10768
7/7 [==============================] - 1s 160ms/step - loss: 0.0606 - root_mean_squared_error: 0.0616 - val_loss: 0.1086 - val_root_mean_squared_error: 0.1086
Epoch 6/40
1/7 [===>..........................] - ETA: 0s - loss: 0.0634 - root_mean_squared_error: 0.06342/7 [=======>......................] - ETA: 0s - loss: 0.0507 - root_mean_squared_error: 0.05233/7 [===========>..................] - ETA: 0s - loss: 0.0513 - root_mean_squared_error: 0.05234/7 [================>.............] - ETA: 0s - loss: 0.0487 - root_mean_squared_error: 0.04985/7 [====================>.........] - ETA: 0s - loss: 0.0483 - root_mean_squared_error: 0.04926/7 [========================>.....] - ETA: 0s - loss: 0.0486 - root_mean_squared_error: 0.04937/7 [==============================] - ETA: 0s - loss: 0.0466 - root_mean_squared_error: 0.0476
Epoch 6: val_loss improved from 0.10768 to 0.10755, saving model to MPRA_model_development/models/MPRAModel.Kampman.mSK_K562.t100.p0.5.c300.v2.conv.16.10.dense.48.dense.1.rmse
7/7 [==============================] - 5s 864ms/step - loss: 0.0466 - root_mean_squared_error: 0.0476 - val_loss: 0.1075 - val_root_mean_squared_error: 0.1075
Epoch 7/40
1/7 [===>..........................] - ETA: 0s - loss: 0.0220 - root_mean_squared_error: 0.02202/7 [=======>......................] - ETA: 0s - loss: 0.0277 - root_mean_squared_error: 0.02833/7 [===========>..................] - ETA: 0s - loss: 0.0341 - root_mean_squared_error: 0.03564/7 [================>.............] - ETA: 0s - loss: 0.0358 - root_mean_squared_error: 0.03705/7 [====================>.........] - ETA: 0s - loss: 0.0367 - root_mean_squared_error: 0.03776/7 [========================>.....] - ETA: 0s - loss: 0.0369 - root_mean_squared_error: 0.03777/7 [==============================] - ETA: 0s - loss: 0.0347 - root_mean_squared_error: 0.0360
Epoch 7: val_loss did not improve from 0.10755
Restoring model weights from the end of the best epoch: 2.
7/7 [==============================] - 1s 163ms/step - loss: 0.0347 - root_mean_squared_error: 0.0360 - val_loss: 0.1085 - val_root_mean_squared_error: 0.1085
Epoch 7: early stopping
TNN loaded from file
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2114, 4)]    0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2114, 4)]    0           []                               
                                                                                                  
 model_1 (Functional)           [(None, 1000),       6377986     ['input_1[0][0]',                
                                 (None, 1)]                       'input_2[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 1000)         0           ['model_1[0][0]',                
                                                                  'model_1[0][1]']                
                                                                                                  
 lambda_1 (Lambda)              (None, 1000)         0           ['model_1[1][0]',                
                                                                  'model_1[1][1]']                
                                                                                                  
 lambda_2 (Lambda)              (None, 1000)         0           ['lambda[0][0]',                 
                                                                  'lambda_1[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 1000, 1)      0           ['lambda_2[0][0]']               
                                                                                                  
 conv1d (Conv1D)                (None, 991, 16)      176         ['tf.expand_dims[0][0]']         
                                                                                                  
 flatten (Flatten)              (None, 15856)        0           ['conv1d[0][0]']                 
                                                                                                  
 dense (Dense)                  (None, 48)           761136      ['flatten[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 1)            49          ['dense[0][0]']                  
                                                                                                  
==================================================================================================
Total params: 7,139,347
Trainable params: 7,139,347
Non-trainable params: 0
__________________________________________________________________________________________________
None
 1/10 [==>...........................] - ETA: 4s 2/10 [=====>........................] - ETA: 0s 3/10 [========>.....................] - ETA: 0s 4/10 [===========>..................] - ETA: 0s 5/10 [==============>...............] - ETA: 0s 6/10 [=================>............] - ETA: 0s 7/10 [====================>.........] - ETA: 0s 8/10 [=======================>......] - ETA: 0s 9/10 [==========================>...] - ETA: 0s10/10 [==============================] - ETA: 0s10/10 [==============================] - 2s 128ms/step
[-0.032189135, -0.11720651, 0.08468147, 0.08749703, -0.312976, -0.07258354, -0.08732863, 0.053688858, -0.046019122, 0.019929582, -0.014724567, -0.089652546, -0.081206895, -0.14632587, -0.042325206, -0.062461656, -0.0901813, -0.013670518, 0.11237505, 0.030217743, 0.0874111, -0.06847825, 0.10198188, 0.040711053, 0.0341401, 0.058246084, 0.07009339, 0.11280145, -0.082539484, 0.008234381, -0.06394619, -0.030837841, -0.0029166571, 0.13782802, 0.08524686, 0.08416666, 0.044592474, -0.025973007, -0.08567629, -0.07934259, -0.067065455, 0.071410276, 0.06888168, -0.11068688, 0.04649481, 0.10051592, 0.040608358, -0.1381066, -0.15001403, -0.06773865, 0.113575324, 0.06261286, 0.052449558, -0.18785211, 0.0125403665, 0.0146946525, 0.08990019, 0.09927112, 0.09721193, -0.08198761, -0.094669096, -0.07215725, -0.011287824, 0.07233377, 0.07682752, 0.06342255, -0.120057404, 0.07969405, -0.08760117, 0.12993164, 0.14611587, 0.09312721, -0.33347934, 0.023950504, -0.10121006, -0.12910102, -0.016633261, 0.04648826, 0.20893477, 0.07235353, -0.101814926, 0.014771442, 0.01264544, 0.08895684, 0.033910427, -0.06991108, -0.11421902, 0.073449485, -0.064665414, 0.09528401, -0.09754857, -0.15148121, 0.058239955, -0.018979615, -0.1260524, 0.14841923, -0.11038192, 0.1421416, -0.059667874, -0.060454186, 0.0650481, -0.14095302, -0.009590204, -0.024294183, 0.009295319, -0.13909276, -0.126135, -0.06582156, 0.095842995, -0.1253093, -0.027726544, -0.13437769, 0.11104025, 0.038992386, -0.116451725, -0.15232216, -0.12956609, 0.040955182, -0.15244462, 0.08347319, -0.12061215, -0.07977976, -0.0018250418, -0.084062114, 0.13677317, 0.15391594, 0.19112442, -0.223003, 0.0013436741, -0.010190156, 0.12597392, 0.24882612, -0.17081355, -0.061621886, -0.13973315, 0.08043061, 0.18703927, -0.03267165, -0.06894755, -0.09217788, 0.20673357, -0.069055244, -0.16832271, -0.24903849, 0.21826248, -0.19228108, -0.31495494, 0.116103195, -0.3475078, -0.3243678, -0.5683379]
[-0.0905034  -0.09947216  0.07853377  0.07739935 -0.26832333 -0.06778263
 -0.08546479  0.08648197  0.07961277  0.07720217 -0.08014117 -0.08105874
 -0.09328441 -0.09695353 -0.11880182 -0.06326722 -0.08434247 -0.09411035
  0.0921365   0.09063613  0.12313518 -0.07950069  0.09031201  0.0710011
 -0.10043573  0.11124648  0.10079366  0.09531208 -0.06277929 -0.11083511
 -0.10710893 -0.11494793  0.13846627  0.11033365  0.09340035  0.09148435
  0.0829305  -0.06689062 -0.07912685 -0.08899292 -0.09257795  0.0991051
  0.08540135 -0.10334674  0.11694617  0.10851879  0.08058769 -0.12419123
 -0.12557973 -0.06900014  0.15094079  0.14292263  0.11062875 -0.16626157
  0.12909251  0.07481506  0.10535965  0.09083616  0.10223201 -0.07872573
 -0.08115115 -0.10202431  0.1065836   0.10569526  0.09134408 -0.10430546
 -0.11229523  0.08098407 -0.07248487  0.10833224  0.11191964  0.09127144
 -0.34156207  0.16014625 -0.10213176 -0.1030441   0.18302456  0.08710979
  0.16541972  0.15007845 -0.11375303 -0.13332473  0.15195966  0.10983172
 -0.12349412 -0.11492698 -0.15814887  0.08052575 -0.11003885  0.08794768
 -0.10758457 -0.14492674  0.11402104 -0.10341273 -0.11568559  0.11976604
 -0.09906955  0.11174096 -0.11254208 -0.1282223   0.0830264  -0.1545139
 -0.1043491  -0.14034679  0.11594063 -0.13771265 -0.16025063 -0.14683859
  0.1183183  -0.1315937  -0.13619249 -0.12484662  0.15976923  0.10062947
 -0.11664557 -0.1552405  -0.17203074  0.1055946  -0.10456964  0.14444356
 -0.11645929 -0.17384205  0.10422415 -0.12774052  0.12609592  0.12372607
  0.23101375 -0.2028399  -0.22582155  0.17918628  0.12624905  0.20775655
 -0.1733779  -0.18529817 -0.1419532   0.16041179  0.15815749 -0.18451102
 -0.19436191 -0.19201529  0.18251119 -0.17792116 -0.17916801 -0.23552507
  0.18936244 -0.22218015 -0.3508249   0.29389578 -0.33032155 -0.29608878
 -0.52658078]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 151 entries, 0 to 150
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   TNN     151 non-null    float32
 1   Y       151 non-null    float64
dtypes: float32(1), float64(1)
memory usage: 1.9 KB
None
         TNN         Y
0  -0.032189 -0.090503
1  -0.117207 -0.099472
2   0.084681  0.078534
3   0.087497  0.077399
4  -0.312976 -0.268323
5  -0.072584 -0.067783
6  -0.087329 -0.085465
7   0.053689  0.086482
8  -0.046019  0.079613
9   0.019930  0.077202
10 -0.014725 -0.080141
11 -0.089653 -0.081059
12 -0.081207 -0.093284
13 -0.146326 -0.096954
14 -0.042325 -0.118802
15 -0.062462 -0.063267
16 -0.090181 -0.084342
17 -0.013671 -0.094110
18  0.112375  0.092136
19  0.030218  0.090636
1/9 [==>...........................] - ETA: 0s2/9 [=====>........................] - ETA: 0s3/9 [=========>....................] - ETA: 0s4/9 [============>.................] - ETA: 0s5/9 [===============>..............] - ETA: 0s6/9 [===================>..........] - ETA: 0s7/9 [======================>.......] - ETA: 0s8/9 [=========================>....] - ETA: 0s9/9 [==============================] - ETA: 0s9/9 [==============================] - 0s 56ms/step
[-0.0901813, 0.0874111, -0.06773865, 0.07969405, 0.20673357, -0.126135, -0.11421902, -0.08567629, -0.13909276, -0.07934259, -0.059667874, -0.24903849, -0.06582156, 0.06261286, -0.1260524, -0.13437769, 0.052449558, -0.11038192, 0.07682752, 0.08749703, 0.09721193, 0.0650481, -0.12061215, -0.081206895, 0.08416666, -0.101814926, 0.10198188, -0.15001403, 0.13782802, 0.19112442, -0.1253093, 0.019929582, 0.11237505, -0.33347934, 0.18703927, -0.013670518, -0.089652546, 0.21826257, 0.038992386, -0.07977976, -0.312976, 0.1421416, -0.032189135, -0.094669096, 0.04648826, 0.073449485, 0.08990019, -0.069055244, -0.067065455, -0.082539484, 0.058246084, 0.030217743, -0.062461656, -0.06847825, 0.13677317, 0.15391594, -0.11068688, 0.040608358, 0.113575324, -0.07258354, -0.15232216, -0.15148121, 0.040955182, -0.5683383, -0.10121006, 0.24882612, 0.04649481, 0.09927112, 0.08043061, 0.06888168, -0.223003, -0.17081355, 0.12993164, -0.027726544, -0.09754857, 0.08895684, -0.18785211, 0.095842995, 0.11280145, 0.07009339, 0.08468147, 0.10051592, 0.08524686, 0.08347319, 0.09312721, -0.08760117, 0.071410276, 0.07233377, -0.120057404, -0.1381066, 0.20893477, -0.07215725, -0.32436797, -0.19228114, -0.06394619, -0.042325206, -0.34750792, -0.060454186, -0.16832271, 0.09528401, 0.058239955, -0.064665414, 0.11104025, 0.12597392, -0.116451725, -0.11720651, -0.08198761, -0.13973315, -0.12910102, -0.0029166571, -0.12956609, 0.008234381, 0.0125403665, 0.0013436741, 0.040711053, -0.024294183, 0.033910427, -0.14632587, 0.14611587, -0.08732863, 0.044592474, 0.116103314, 0.07235353, -0.025973007, -0.014724567, -0.011287824, -0.009590204, 0.014771442, 0.06342252, -0.15244454, 0.014694636, 0.009295302, -0.018979616, -0.061621863, -0.092177846]
[-0.08434247  0.12313518 -0.06900014  0.08098407  0.18251119 -0.16025063
 -0.15814887 -0.07912685 -0.13771265 -0.08899292 -0.11254208 -0.23552507
 -0.14683859  0.14292263 -0.11568559 -0.12484662  0.11062875 -0.09906955
  0.09134408  0.07739935  0.10223201  0.0830264  -0.11645929 -0.09328441
  0.09148435 -0.11375303  0.09031201 -0.12557973  0.11033365  0.23101375
 -0.1315937   0.07720217  0.0921365  -0.34156207  0.15815749 -0.09411035
 -0.08105874  0.18936244  0.10062947 -0.17384205 -0.26832333  0.11174096
 -0.0905034  -0.08115115  0.08710979  0.08052575  0.10535965 -0.17792116
 -0.09257795 -0.06277929  0.11124648  0.09063613 -0.06326722 -0.07950069
  0.12609592  0.12372607 -0.10334674  0.08058769  0.15094079 -0.06778263
 -0.1552405  -0.14492674  0.1055946  -0.52658078 -0.10213176  0.20775655
  0.11694617  0.09083616  0.16041179  0.08540135 -0.2028399  -0.1733779
  0.10833224 -0.13619249 -0.10758457  0.10983172 -0.16626157  0.1183183
  0.09531208  0.10079366  0.07853377  0.10851879  0.09340035  0.14444356
  0.09127144 -0.07248487  0.0991051   0.10569526 -0.11229523 -0.12419123
  0.16541972 -0.10202431 -0.29608878 -0.22218015 -0.10710893 -0.11880182
 -0.33032155 -0.1282223  -0.17916801  0.08794768  0.11402104 -0.11003885
  0.15976923  0.12624905 -0.11664557 -0.09947216 -0.07872573 -0.1419532
 -0.1030441   0.13846627 -0.17203074 -0.11083511  0.12909251 -0.22582155
  0.0710011  -0.14034679 -0.12349412 -0.09695353  0.11191964 -0.08546479
  0.0829305   0.29389578  0.15007845 -0.06689062 -0.08014117  0.1065836
 -0.1043491  -0.13332473 -0.10430546 -0.10456964  0.07481506  0.11594063
 -0.10341273 -0.18529817 -0.19201529]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 135 entries, 0 to 134
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   TNN     135 non-null    float32
 1   Y       135 non-null    float64
dtypes: float32(1), float64(1)
memory usage: 1.7 KB
None
         TNN         Y
0  -0.090181 -0.084342
1   0.087411  0.123135
2  -0.067739 -0.069000
3   0.079694  0.080984
4   0.206734  0.182511
5  -0.126135 -0.160251
6  -0.114219 -0.158149
7  -0.085676 -0.079127
8  -0.139093 -0.137713
9  -0.079343 -0.088993
10 -0.059668 -0.112542
11 -0.249038 -0.235525
12 -0.065822 -0.146839
13  0.062613  0.142923
14 -0.126052 -0.115686
15 -0.134378 -0.124847
16  0.052450  0.110629
17 -0.110382 -0.099070
18  0.076828  0.091344
19  0.087497  0.077399
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 352ms/step
[0.053688858, -0.016633261, -0.03267165, -0.31495503, -0.010190156, -0.030837841, 0.14841923, 0.01264544, 0.023950504, 0.0341401, -0.06991108, -0.06894755, -0.046019122, -0.084062114, -0.0018250418, -0.14095302]
[ 0.08648197  0.18302456 -0.18451102 -0.3508249   0.17918628 -0.11494793
  0.11976604  0.15195966  0.16014625 -0.10043573 -0.11492698 -0.19436191
  0.07961277 -0.12774052  0.10422415 -0.1545139 ]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 16 entries, 0 to 15
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   TNN     16 non-null     float32
 1   Y       16 non-null     float64
dtypes: float32(1), float64(1)
memory usage: 320.0 bytes
None
         TNN         Y
0   0.053689  0.086482
1  -0.016633  0.183025
2  -0.032672 -0.184511
3  -0.314955 -0.350825
4  -0.010190  0.179186
5  -0.030838 -0.114948
6   0.148419  0.119766
7   0.012645  0.151960
8   0.023951  0.160146
9   0.034140 -0.100436
10 -0.069911 -0.114927
11 -0.068948 -0.194362
12 -0.046019  0.079613
13 -0.084062 -0.127741
14 -0.001825  0.104224
15 -0.140953 -0.154514
spearman TNN-all: 0.8244126873475078
pearson TNN-all: 0.8839934619274112
spearman TNN-train: 0.851765681396937
pearson TNN-train: 0.9091788691518505
spearman TNN-test: 0.6941176470588235
pearson TNN-test: 0.7227690947765518
